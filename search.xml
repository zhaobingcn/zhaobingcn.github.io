<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[node快速入门]]></title>
    <url>%2F2017%2F08%2F24%2Fnode%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[CommonJS规范CommonJS对模块的定义十分简单，主要分为模块引用，模块定义和模块标识三个部分。 模块引用模块引用的示例代码如下：1var math = require(&quot;math&quot;) 即可以引入一个模块的API到当前上下文。 模块定义在模块中，上下文提供了require()来引入外部模块。对应引入的功能，]]></content>
      <tags>
        <tag>-nodeJS -后端开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法之动态规划-硬币问题]]></title>
    <url>%2F2017%2F08%2F23%2F%E7%AE%97%E6%B3%95%E4%B9%8B%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92-%E7%A1%AC%E5%B8%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[#从硬币问题开始]]></content>
      <tags>
        <tag>-算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tornado]]></title>
    <url>%2F2017%2F08%2F10%2FTornado%2F</url>
    <content type="text"><![CDATA[最近需要使用python做后台研发，因此学习了下python tornado相关的知识，在这里记录下来方便以后回顾。 Tornado简介Tornado是使用Python编写的一个强大的、可扩展的Web服务器。它在处理严峻的网络流量时表现得足够强健，但却在创建和编写时有着足够的轻量级，并能够被用在大量的应用和工具中。 第一个Tornado示例1234567891011121314import tornado.ioloopimport tornado.webclass MainHandler(tornado.web.RequestHandler): def get(self): self.write("Hello, world")application = tornado.web.Application([ (r"/", MainHandler),])if __name__ == "__main__": application.listen(8888) tornado.ioloop.IOLoop.instance().start() 在浏览器输入localhost:8888即可以得到hello world的输出，这是一个最简单的发出http get请求，并且返回字符串的例子。 内建方法get_argument1234class StoryHandler(tornado.web.RequestHandler): def get(self): name = self.get_argument("name", "admin"); self.write("you request the story " + name); 第一个参数为我们定义的参数形参名，第二个参数为默认参数，如果我们传递参数将使用第二个参数作为默认值。 参数handlers123application = tornado.web.Application([ (r"/", MainHandler),]) 这里的参数handle非常重要，值得我们更加深入的研究。他是元组组成的一个列表，其中第一个参数是解析url的正则表达式，第二个参数是RequestHander类。 字符串服务12345678910111213141516171819202122232425262728293031import textwrapimport tornado.httpserverimport tornado.ioloopimport tornado.optionsimport tornado.webfrom tornado.options import define, optionsdefine("port", default=8000, help="run on the given port", type=int)class ReverseHandler(tornado.web.RequestHandler): def get(self, input): self.write(input[::-1])class WrapHandler(tornado.web.RequestHandler): def post(self): text = self.get_argument('text') width = self.get_argument('width', 40) self.write(textwrap.fill(text, int(width))) if __name__ == "__main__": tornado.options.parse_command_line() app = tornado.web.Application( handlers=[ (r"/reverse/(\w+)", ReverseHandler), (r"/wrap", WrapHandler) ] ) http_server = tornado.httpserver.HTTPServer(app) http_server.listen(options.port) tornado.ioloop.IOLoop.instance().start() 这里有两个Handler，第一个是将字符串翻转，第二个是输入参数为字符串，并且在一定的宽度范围内显示。 你可以看到这里的get方法有一个额外的参数input。这个参数将包含匹配处理函数正则表达式第一个括号里的字符串。（如果正则表达式中有一系列额外的括号，匹配的字符串将被按照在正则表达式中出现的顺序作为额外的参数传递进来。） HTTP方法一个Requesthandler类里面可以有多个HTTP方法，虽然上面我们只定义了一种HTTP方法，但是实际中我们是可以定义多种HTTP方法的。包括（GET,POST,PUT,DELETE,HEAD,OPTION）等等。下面一个就是定义了GET与POST方法的RequestHandler类。123456789class WidgetHandler(tornado.web.RequestHandler): def get(self, widget_id): widget = retrieve_from_db(widget_id) self.write(widget.serialize()) def post(self, widget_id): widget = retrieve_from_db(widget_id) widget['foo'] = self.get_argument('foo') save_to_db(widget) 模板和表单模仿一个表单的提交请求，并且获取得到的参数值1234567891011121314151617181920212223242526272829303132import os.pathimport tornado.httpserverimport tornado.ioloopimport tornado.optionsimport tornado.webfrom tornado.options import define, optionsdefine("port", default=8000, help="run on the given port", type=int)class IndexHandler(tornado.web.RequestHandler): def get(self): self.render('index.html')class PoemPageHandler(tornado.web.RequestHandler): def post(self): noun1 = self.get_argument('noun1') noun2 = self.get_argument('noun2') verb = self.get_argument('verb') noun3 = self.get_argument('noun3') self.render('poem.html', roads=noun1, wood=noun2, made=verb, difference=noun3)if __name__ == '__main__': tornado.options.parse_command_line() app = tornado.web.Application( handlers=[(r'/', IndexHandler), (r'/poem', PoemPageHandler)], template_path=os.path.join(os.path.dirname(__file__), "../templates") ) http_server = tornado.httpserver.HTTPServer(app) http_server.listen(options.port) tornado.ioloop.IOLoop.instance().start() index.html页面，包括form表单，用的是post请求。1234567891011121314&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt;&lt;title&gt;Poem Maker Pro&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;h1&gt;Enter terms below.&lt;/h1&gt; &lt;form method="post" action="/poem"&gt; &lt;p&gt;Plural noun&lt;br&gt;&lt;input type="text" name="noun1"&gt;&lt;/p&gt; &lt;p&gt;Singular noun&lt;br&gt;&lt;input type="text" name="noun2"&gt;&lt;/p&gt; &lt;p&gt;Verb (past tense)&lt;br&gt;&lt;input type="text" name="verb"&gt;&lt;/p&gt; &lt;p&gt;Noun&lt;br&gt;&lt;input type="text" name="noun3"&gt;&lt;/p&gt; &lt;input type="submit"&gt; &lt;/form&gt; &lt;/body&gt;&lt;/html&gt; poem页面，即模板页面，使用得到的数据填充，填充一个参数的标识符是{}12345678910&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt;&lt;title&gt;Poem Maker Pro&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;h1&gt;Your poem&lt;/h1&gt; &lt;p&gt;Two &#123;&#123;roads&#125;&#125; diverged in a &#123;&#123;wood&#125;&#125;, and I—&lt;br&gt;I took the one less travelled by,&lt;br&gt;And that has &#123;&#123;made&#125;&#125; all the &#123;&#123;difference&#125;&#125;.&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; self.render会开始渲染模板，但是这个模板中没有任何我们传入的参数，下面的poem页面即有我们传入参数的模板渲染，这个叫填充。]]></content>
      <tags>
        <tag>Python</tag>
        <tag>Python Web</tag>
        <tag>WSGI</tag>
        <tag>Tornado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图计算最新进展]]></title>
    <url>%2F2017%2F07%2F27%2F%E5%9B%BE%E8%AE%A1%E7%AE%97%E6%9C%80%E6%96%B0%E8%BF%9B%E5%B1%95%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Graph</tag>
        <tag>图计算</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot项目部署]]></title>
    <url>%2F2017%2F07%2F25%2FSpringBoot%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[常见问题使用Spring Boot构建项目的时候因为默认构建的包为jar包，因此会出现一些部署上的困惑，在这里记录下来Sping Boot程序的部署过程，方便之后回顾。 修改打包形式在pom.xml中设置打包格式war 移除服务器已有的插件如果构建的是web程序，则需要排除tomcat。 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;!-- 移除嵌入式tomcat插件 --&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 添加servlet-api的依赖下面两种方法都可以，任选其一123456&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 第二种方法123456&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-servlet-api&lt;/artifactId&gt; &lt;version&gt;8.0.36&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 修改启动类，并且重写初始化方法我们常用的是用main方法启动，代码如下：123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 在这里我们需要使用类似web.xml的配制方法来启动spring上下文了，在Application的同级目录下添加SpringBootStartApplication类： 1234567891011/** * 修改启动类，继承 SpringBootServletInitializer 并重写 configure 方法 */public class SpringBootStartApplication extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; // 注意这里要指向原先用main方法执行的Application启动类 return builder.sources(Application.class); &#125;&#125; 打包部署然后需要使用mvn package命令将项目打包，把打包好的项目移动到tomcat或jetty的webapp目录下。重新启动jetty或者tomcat. 1http://localhost:[端口号]/[项目名]]]></content>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TinkerPop学习]]></title>
    <url>%2F2017%2F07%2F17%2FTinkerPop%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[TinkerPop简介 TinkerPop由一系列的共同操作组件组成。他最基本的API定义了如何实现一个图，节点，边等等操作。最简单的实现图操作的方法是实现核心api。用户可以使用Gremlin语言进行图形操作。而且还提供了更高级的图形操作，实现实时查询，优化表现性能（索引机制等）。如果是一个图处理系统（OLAP），可以实现GraphComputer API。这些API定义了消息是如何在不同的工作节点之上进行通信的。同样的Gremlin遍历可以实现在图数据库(OLTP)和图处理(OLAP)系统中。Gremlin是一个基于图的DSL语言。 一些图的指标 Graph Features graph.features(),表示当前图支持的性质。 Vertex Properties 1.一个属性支持多个值，多个值用数组表示 2.属性上面还支持key/value对表示的属性 12345678910111213141516gremlin&gt; g.V().as('a'). properties('location').as('b'). hasNot('endTime').as('c'). select('a','b','c').by('name').by(value).by('startTime') // determine the current location of each person==&gt;[a:marko,b:santa fe,c:2005]==&gt;[a:stephen,b:purcellville,c:2006]==&gt;[a:matthias,b:seattle,c:2014]==&gt;[a:daniel,b:aachen,c:2009]gremlin&gt; g.V().has('name','gremlin').inE('uses'). order().by('skill',incr).as('a'). outV().as('b'). select('a','b').by('skill').by('name') // rank the users of gremlin by their skill level==&gt;[a:3,b:matthias]==&gt;[a:4,b:marko]==&gt;[a:5,b:stephen]==&gt;[a:5,b:daniel] 注意as， select， by的使用方法。 Graph Variables 存储了关于数据库的元数据信息 Graph Transactions]]></content>
      <tags>
        <tag>TinkerPop</tag>
        <tag>Graph</tag>
        <tag>Janusgraph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka基本原理]]></title>
    <url>%2F2017%2F04%2F23%2FKafka%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Kafka简介简介 Kafka提供了类似JMS（Java Message Service）的特性，但是在设计实现上完全不同，此外，它并不是JMS规范的实现。Kafka对消息保存时根据Topic分类，发送消息的是Producer接收消息的是Consumer，Kafka集群由多个Kafka实例组成，每个实例被称作Broker。无论是Kafka集群，还是Producder和Consumer都依赖于Zookeeper保证系统的可用性和保存系统的Meta数据。 Kafka中的常见名词 1.producer消息的生产者，发送消息到Kafka集群或者终端的服务2.broker集群中包含的服务器3.topic每条发送到kafka中消息的类别，即kafka的消息是面向topic的4.partition是物理上的概念，每个topic包含一个或者多个partition。kafka的分配单位是partition5.consumer从kafka集群中消费消息的终端或者服务6.consumer group每个consumer都属于一个consumer group，每条消息只能被consumer group中的一个consumer消费，但是可以被多个consumer group消费7.replicapartition中的副本，用来保证这部分数据的高可用性8.leaderreplica中的一个角色，负责消息的读和写，producer和consumer只和leader交互9.followerreplica中的一个角色，从leader复制数据，为了保证高可用性，leader故障时可以成为leader10.controllerkafka集群中的一个服务器，用来进行leader election和这种failover11.zookeeper协调kafka中的producer，consumer以及保存kafka中的meta数据 拓扑结构如下图： 基本性问题介绍一个Topic可以认为是一类消息，每个Topic将会被分为多个partition（分区），每个partition在存储层面是append log。任何发布到此partition的消息都会被追加到log文件的尾部，每条消息在文件中的位置称为offset（偏移量），offset是一个long型数字，他唯一标记一条信息的位置。kafka并没有提供其他的索引机制来存储offset，因此在Kafka中几乎不允许对消息进行”随机读写”。Kafka和JMS的实现（ActiveMQ）不同的是：即使消息被消费，消息仍然不会被立即删除。日志文件将会根据broker中的配置要求，保留一段时间以后删除，一种是保留一段时间后删除，比如说保留两天，另一种是当消息队列超过一定大小时，而不管消息是否被消费过。Kafka通过这种简单的手段来释放磁盘空间，以及减少消息消费之后文件改动的磁盘IO开销。 对于consumer而言，它需要保存的是消息的offset。当消息正常消费时，offset会线性的向前驱动，即消息将依次被消费。事实上consumer可以使用任意顺序消费消息，只需要他将offset设置为任意值。（offset将会被保存在zookeeper中） kafka集群几乎不需要维护任何producer和consumer状态信息，这些信息由zookeeper保存。因此producer和consumer的客户端实现非常轻量级，他们可以随意的离开而不会对集群造成影响。 partition的设计目的有多个，其中最主要的原因是kafka通过文件存储，通过分区可以将日志文件分散到多个server上，避免文件尺寸达到单机存储的上限，每个partitio都会被当前服务器保存。follower只是复制leader的状态，作为leader的服务器承载了全部的请求压力，因此从集群的整体考虑，有多少个partitions就有多少个leader，kafka可以将leader均衡的分散在不同的节点上分担负载压力。 producer发布消息写入方式producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。 消息路由producer产生消息发送到broker时，会根据一定的算法选择将其存储到哪个partition。其路由机制为：123指定了 patition，则直接使用；未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patitionpatition 和 key 都未指定，使用轮询选出一个 patition。 写入流程producer 写入消息序列图如下所示：写入具体流程说明12345producer 先从 zookeeper 的 &quot;/brokers/.../state&quot; 节点找到该 partition 的 leaderproducer 将消息发送给该 leaderleader 将消息写入本地 logfollowers 从 leader pull 消息，写入本地 log 后 leader 发送 ACKleader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK 消息传输保证（producer delivery guarantee）一般存在下面三种情况123At most once 消息可能会丢，但绝不会重复传输At least one 消息绝不会丢，但可能会重复传输Exactly once 每条消息肯定会被传输一次且仅传输一次 当 producer 向 broker 发送消息时，一旦这条消息被 commit，由于 replication 的存在，它就不会丢。但是如果 producer 发送数据给 broker 后，遇到网络问题而造成通信中断，那 Producer 就无法判断该条消息是否已经 commit。虽然 Kafka 无法确定网络故障期间发生了什么，但是 producer 可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，这样就做到了 Exactly once，但目前还并未实现。所以目前默认情况下一条消息从 producer 到 broker 是确保了 At least once，可通过设置 producer 异步发送实现At most once。 broker保存消息存储方式物理上把topic分成一个或者多个partition，每个partition物理上对应一个文件夹（该文件夹存储该partition的所有索引和消息文件），如下： 存储策略无论消息是否被消费，kafka 都会保留所有消息。有两种策略可以删除旧数据：12基于时间：log.retention.hours=168基于大小：log.retention.bytes=1073741824 需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。 topic的创建与删除创建topic创建topic的序列图如下所示：流程说明：123451. controller 在 ZooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。2. controller从 /brokers/ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition： 2.1 从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的 ISR 2.2 将新的 leader 和 ISR 写入 /brokers/topics/[topic]/partitions/[partition]/state3. controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。 删除topic删除topic的序列图如下所示：12controller 在 zooKeeper 的 /brokers/topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition/replica 分配。若 delete.topic.enable=false，结束；否则 controller 注册在 /admin/delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。 高可用replication一个Topic可以有多个partitions，被分布在kafka集群中的多个server上。kafka可以配置partition需要备份的个数（replicas），每个partition会被备份到多台机器上，以提高可用性。当一台机器宕机时，其他机器上的副本仍然可以提供服务。 每个partition都会有多个备份，其中一个是leader，其他的都是follower。leader执行所有的读写操作，follower只是跟进leader的状态。 kafka分配replica的算法如下123将所有的broker(假设总共有n个broker）和待分配的partition排序将第i个partition分配到第（i mod n）个broker上将第i个partition的第j个replica分配到第（（i+j）mod n）个broker上 leader failover当partition对应的leader宕机时，需要从follower中选举出新leader。在选举新leader时，一个基本的原则是，新的leader必须拥有旧的 leader commit 过的所有消息。 kafka在zookeeper中（/brokers/…/state）动态维护了一个ISR（in-sync replicas），由3.3节的写入流程可知 ISR 里面的所有 replica 都跟上了 leader，只有 ISR 里面的成员才能选为 leader。对于 f+1 个 replica，一个 partition 可以在容忍 f 个 replica 失效的情况下保证消息不丢失。 当所有 replica 都不工作时，有两种可行的方案： 等待 ISR 中的任一个 replica 活过来，并选它作为 leader。可保障数据不丢失，但时间可能相对较长。 选择第一个活过来的 replica（不一定是 ISR 成员）作为 leader。无法保障数据不丢失，但相对不可用时间较短。kafka 0.8.* 使用第二种方式。 kafka 通过 Controller 来选举 leader，流程请参考5.3节。 broker failoverkafka broker failover 序列图如下所示：流程说明：123456781. controller 在 zookeeper 的 /brokers/ids/[brokerId] 节点注册 Watcher，当 broker 宕机时 zookeeper 会 fire watch2. controller 从 /brokers/ids 节点读取可用broker3. controller决定set_p，该集合包含宕机 broker 上的所有 partition4. 对 set_p 中的每一个 partition 4.1 从/brokers/topics/[topic]/partitions/[partition]/state 节点读取 ISR 4.2 决定新 leader（如4.3节所描述） 4.3 将新 leader、ISR、controller_epoch 和 leader_epoch 等信息写入 state 节点5. 通过 RPC 向相关 broker 发送 leaderAndISRRequest 命令 controller failover当 controller 宕机时会触发 controller failover。每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失，所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。 当新的 controller 当选时，会触发 KafkaController.onControllerFailover 方法，在该方法中完成如下操作： 123456789101112读取并增加 Controller Epoch。在 reassignedPartitions Patch(/admin/reassign_partitions) 上注册 watcher。在 preferredReplicaElection Path(/admin/preferred_replica_election) 上注册 watcher。通过 partitionStateMachine 在 broker Topics Patch(/brokers/topics) 上注册 watcher。若 delete.topic.enable=true（默认值是 false），则 partitionStateMachine 在 Delete Topic Patch(/admin/delete_topics) 上注册 watcher。通过 replicaStateMachine在 Broker Ids Patch(/brokers/ids)上注册Watch。初始化 ControllerContext 对象，设置当前所有 topic，“活”着的 broker 列表，所有 partition 的 leader 及 ISR等。启动 replicaStateMachine 和 partitionStateMachine。将 brokerState 状态设置为 RunningAsController。将每个 partition 的 Leadership 信息发送给所有“活”着的 broker。若 auto.leader.rebalance.enable=true（默认值是true），则启动 partition-rebalance 线程。若 delete.topic.enable=true 且Delete Topic Patch(/admin/delete_topics)中有值，则删除相应的Topic。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
        <tag>消息系统</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ学习]]></title>
    <url>%2F2017%2F03%2F27%2FRabbitMQ%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[你是否遇到过两个（多个）系统间需要通过定时任务来同步某些数据？你是否在为异构系统的不同进程间相互调用、通讯的问题而苦恼、挣扎？如果是，那么恭喜你，消息服务让你可以很轻松地解决这些问题。消息服务擅长于解决多系统、异构系统间的数据交换（消息通知/通讯）问题，你也可以把它用于系统间服务的相互调用（RPC）。本文将要介绍的RabbitMQ就是当前最主流的消息中间件之一。 RabbitMQ简介AMQP，即Advanced Message Queuing Protocol，高级消息队列协议，是应用层协议的一个开放标准，为面向消息的中间件设计。消息中间件主要用于组件之间的解耦，消息的发送者无需知道消息使用者的存在，反之亦然。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布/订阅）、可靠性、安全。RabbitMQ是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。下面将重点介绍RabbitMQ中的一些基础概念，了解了这些概念，是使用好RabbitMQ的基础。 ConnectionFactory、Connection、ChannelConnectionFactory、Connection、Channel都是RabbitMQ对外提供的API中最基本的对象。Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。ConnectionFactory为Connection的制造工厂。Channel是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 QueueQueue（队列）是RabbitMQ的内部对象，用于存储消息，用下图表示。RabbitMQ中的消息都只能存储在Queue中，生产者（下图中的P）生产消息并最终投递到Queue中，消费者（下图中的C）可以从Queue中获取消息并消费。多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 Message acknowledgment在实际应用中，可能会发生消费者收到Queue中的消息，但没有处理完成就宕机（或出现其他意外）的情况，这种情况下就可能会导致消息丢失。为了避免这种情况发生，我们可以要求消费者在消费完消息后发送一个回执给RabbitMQ，RabbitMQ收到消息回执（Message acknowledgment）后才将该消息从Queue中移除；如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）进行处理。这里不存在timeout概念，一个消费者处理消息时间再长也不会导致该消息被发送给其他消费者，除非它的RabbitMQ连接断开。这里会产生另外一个问题，如果我们的开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致严重的bug——Queue中堆积的消息会越来越多；消费者重启后会重复消费这些消息并重复执行业务逻辑… 另外pub message是没有ack的。 Message durability如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。 Prefetch count前面我们讲到如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平摊给多个消费者。这时如果每个消息的处理时间不同，就有可能会导致某些消费者一直在忙，而另外一些消费者很快就处理完手头工作并一直空闲的情况。我们可以通过设置prefetchCount来限制Queue每次发送给每个消费者的消息数，比如我们设置prefetchCount=1，则Queue每次给每个消费者发送一条消息；消费者处理完这条消息后Queue会再给该消费者发送一条消息。 Exchange在上一节我们看到生产者将消息投递到Queue中，实际上这在RabbitMQ中这种事情永远都不会发生。实际的情况是，生产者将消息发送到Exchange（交换器，下图中的X），由Exchange将消息路由到一个或多个Queue中（或者丢弃）。 Exchange是按照什么逻辑将消息路由到Queue的？这个将在Binding一节介绍。RabbitMQ中的Exchange有四种类型，不同的类型有着不同的路由策略，这将在Exchange Types一节介绍。 routing key生产者在将消息发送给Exchange的时候，一般会指定一个routing key，来指定这个消息的路由规则，而这个routing key需要与Exchange Type及binding key联合使用才能最终生效。在Exchange Type与binding key固定的情况下（在正常使用时一般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过指定routing key来决定消息流向哪里。RabbitMQ为routing key设定的长度限制为255 bytes。 BindingRabbitMQ中通过Binding将Exchange与Queue关联起来，这样RabbitMQ就知道如何正确地将消息路由到指定的Queue了。 Binding key在绑定（Binding）Exchange与Queue的同时，一般会指定一个binding key；消费者将消息发送给Exchange时，一般会指定一个routing key；当binding key与routing key相匹配时，消息将会被路由到对应的Queue中。这个将在Exchange Types章节会列举实际的例子加以说明。在绑定多个Queue到同一个Exchange的时候，这些Binding允许使用相同的binding key。binding key 并不是在所有情况下都生效，它依赖于Exchange Type，比如fanout类型的Exchange就会无视binding key，而是将消息路由到所有绑定到该Exchange的Queue。 Exchaneg TypesRabbitMQ常用的Exchange Type有fanout、direct、topic、headers这四种（AMQP规范里还提到两种Exchange Type，分别为system与自定义，这里不予以描述），下面分别进行介绍。 fanoutfanout类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中。 directdirect类型的Exchange路由规则也很简单，它会把消息路由到那些binding key与routing key完全匹配的Queue中。 以上图的配置为例，我们以routingKey=”error”发送消息到Exchange，则消息会路由到Queue1（amqp.gen-S9b…，这是由RabbitMQ自动生成的Queue名称）和Queue2（amqp.gen-Agl…）；如果我们以routingKey=”info”或routingKey=”warning”来发送消息，则消息只会路由到Queue2。如果我们以其他routingKey发送消息，则消息不会路由到这两个Queue中。 topic前面讲到direct类型的Exchange路由规则是完全匹配binding key与routing key，但这种严格的匹配方式在很多情况下不能满足实际业务需求。topic类型的Exchange在匹配规则上进行了扩展，它与direct类型的Exchage相似，也是将消息路由到binding key与routing key相匹配的Queue中，但这里的匹配规则有些不同，它约定： routing key为一个句点号“. ”分隔的字符串（我们将被句点号“. ”分隔开的每一段独立的字符串称为一个单词），如“stock.usd.nyse”、“nyse.vmw”、“quick.orange.rabbit” binding key与routing key一样也是句点号“. ”分隔的字符串 binding key中可以存在两种特殊字符“”与“#”，用于做模糊匹配，其中“”用于匹配一个单词，“#”用于匹配多个单词（可以是零个） 以上图中的配置为例，routingKey=”quick.orange.rabbit”的消息会同时路由到Q1与Q2，routingKey=”lazy.orange.fox”的消息会路由到Q1与Q2，routingKey=”lazy.brown.fox”的消息会路由到Q2，routingKey=”lazy.pink.rabbit”的消息会路由到Q2（只会投递给Q2一次，虽然这个routingKey与Q2的两个bindingKey都匹配）；routingKey=”quick.brown.fox”、routingKey=”orange”、routingKey=”quick.orange.male.rabbit”的消息将会被丢弃，因为它们没有匹配任何bindingKey。 headersheaders类型的Exchange不依赖于routing key与binding key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。在绑定Queue与Exchange时指定一组键值对；当消息发送到Exchange时，RabbitMQ会取到该消息的headers（也是一个键值对的形式），对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对；如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。该类型的Exchange没有用到过（不过也应该很有用武之地），所以不做介绍。 RPCMQ本身是基于异步的消息处理，前面的示例中所有的生产者（P）将消息发送到RabbitMQ后不会知道消费者（C）处理成功或者失败（甚至连有没有消费者来处理这条消息都不知道）。但实际的应用场景中，我们很可能需要一些同步处理，需要同步等待服务端将我的消息处理完成后再进行下一步处理。这相当于RPC（Remote Procedure Call，远程过程调用）。在RabbitMQ中也支持RPC。RabbitMQ中实现RPC的机制是： 客户端发送请求（消息）时，在消息的属性（MessageProperties，在AMQP协议中定义了14中properties，这些属性会随着消息一起发送）中设置两个值replyTo（一个Queue名称，用于告诉服务器处理完成后将通知我的消息发送到这个Queue中）和correlationId（此次请求的标识号，服务器处理完成后需要将此属性返还，客户端将根据这个id了解哪条请求被成功执行了或执行失败） 服务器端收到消息并处理 服务器端处理完消息后，将生成一条应答消息到replyTo指定的Queue，同时带上correlationId属性 客户端之前已订阅replyTo指定的Queue，从中收到服务器的应答消息后，根据其中的correlationId属性分析哪条请求被执行了，根据执行结果进行后续业务处理]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>RabbitMQ</tag>
        <tag>消息队列</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySql优化的问题]]></title>
    <url>%2F2017%2F03%2F08%2FMySql%E4%BC%98%E5%8C%96%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Mysql优化的几条原则一、第一优化你的sql和索引；二、第二加缓存，memcache,redis；三、第三以上都做了后，还是慢，就做主从复制或主主复制，读写分离，可以在应用层做，效率高，也可以用三方工具，第三方工具推荐360的atlas,其它的要么效率不高，要么没人维护；四、第四如果以上都做了还是慢，不要想着去做切分，mysql自带分区表，先试试这个，对你的应用是透明的，无需更改代码,但是sql语句是需要针对分区表做优化的，sql条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区，另外分区表还有一些坑，在这里就不多说了；五、如果以上都做了，那就先做垂直拆分，其实就是根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；六、第六才是水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key,为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； mysql数据库一般都是按照这个步骤去演化的，成本也是由低到高； 有人也许要说第一步优化sql和索引这还用说吗？的确，大家都知道，但是很多情况下，这一步做的并不到位，甚至有的只做了根据sql去建索引，根本没对sql优化（中枪了没？），除了最简单的增删改查外，想实现一个查询，可以写出很多种查询语句，不同的语句，根据你选择的引擎、表中数据的分布情况、索引情况、数据库优化策略、查询中的锁策略等因素，最终查询的效率相差很大；优化要从整体去考虑，有时你优化一条语句后，其它查询反而效率被降低了，所以要取一个平衡点；即使精通mysql的话，除了纯技术面优化，还要根据业务面去优化sql语句，这样才能达到最优效果；你敢说你的sql和索引已经是最优了吗? 再说一下不同引擎的优化，myisam读的效果好，写的效率差，这和它数据存储格式，索引的指针和锁的策略有关的，它的数据是顺序存储的（innodb数据存储方式是聚簇索引），他的索引btree上的节点是一个指向数据物理位置的指针，所以查找起来很快，（innodb索引节点存的则是数据的主键，所以需要根据主键二次查找）；myisam锁是表锁，只有读读之间是并发的，写写之间和读写之间（读和插入之间是可以并发的，去设置concurrent_insert参数，定期执行表优化操作，更新操作就没有办法了）是串行的，所以写起来慢，并且默认的写优先级比读优先级高，高到写操作来了后，可以马上插入到读操作前面去，如果批量写，会导致读请求饿死，所以要设置读写优先级或设置多少写操作后执行读操作的策略;myisam不要使用查询时间太长的sql，如果策略使用不当，也会导致写饿死，所以尽量去拆分查询效率低的sql, innodb一般都是行锁，这个一般指的是sql用到索引的时候，行锁是加在索引上的，不是加在数据记录上的，如果sql没有用到索引，仍然会锁定表,mysql的读写之间是可以并发的，普通的select是不需要锁的，当查询的记录遇到锁时，用的是一致性的非锁定快照读，也就是根据数据库隔离级别策略，会去读被锁定行的快照，其它更新或加锁读语句用的是当前读，读取原始行；因为普通读与写不冲突，所以innodb不会出现读写饿死的情况，又因为在使用索引的时候用的是行锁，锁的粒度小，竞争相同锁的情况就少，就增加了并发处理，所以并发读写的效率还是很优秀的，问题在于索引查询后的根据主键的二次查找导致效率低； ps:很奇怪，为什innodb的索引叶子节点存的是主键而不是像mysism一样存数据的物理地址指针吗？如果存的是物理地址指针不就不需要二次查找了吗，这也是我开始的疑惑，根据mysism和innodb数据存储方式的差异去想，你就会明白了，我就不费口舌了！ 所以innodb为了避免二次查找可以使用索引覆盖技术，无法使用索引覆盖的，再延伸一下就是基于索引覆盖实现延迟关联；不知道什么是索引覆盖的，建议你无论如何都要弄清楚它是怎么回事！ 尽你所能去优化你的sql吧！说它成本低，却又是一项费时费力的活，需要在技术与业务都熟悉的情况下，用心去优化才能做到最优，优化后的效果也是立竿见影的！]]></content>
      <categories>
        <category>MySql</category>
      </categories>
      <tags>
        <tag>MySql</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j集群搭建]]></title>
    <url>%2F2016%2F12%2F09%2FNeo4j%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Neo4j集群的功能Neo4j可以在集群模式下配置，以适应不同的负载，容错和可用硬件的要求。Neo4j集群由一个Matser实例和零个或多个Slave实例组成。 集群中的所有实例都在其本地数据库文件中具有数据的完整副本。 一个具有三个实例的Neo4j集群如图所示： 上图中的绿色箭头代表的是，每个实例包含所需的逻辑，以便与集群中的其他成员协调以进行数据复制和选举管理。上图中的蓝色箭头代表的是，Slave实例（不包括atbiter实例）和Master实例之间的相互通信，以确保每个每个数据库中的数据都是最新的。 Neo4j集群的具体搭建过程安装环境 虚拟机系统：UbuntuServer 16.04 虚拟机配置：内存2G， CPU2P Jdk: 1.8 Neo4j: 3.0.7企业版 安装步骤添加IP和主机名称 在每个虚拟机上打开hosts文件1vi /etc/hosts 分别在每个虚拟机的配置文件中添加三个虚拟机的IP和名称12310.108.217.243 Neo4j-0110.108.219.44 Neo4j-0210.108.217.230 Neo4j-03 关闭防火墙(没有的话则不必管)1iptables off 为每个虚拟机安装Neo4j 我使用root用户安装的，如果用其他用户安装也可以。 创建Neo4j的目录1mkdir /usr/neo4j 将下载的neo4j-enterprise-3.0.7-unix.tar.gz文件放到刚才的目录中 解压安装包12tar -xf /usr/neo4j/neo4j-enterprise-3.0.7-unix.tar.gzrm /usr/neo4j/neo4j-enterprise-3.0.7-unix.tar.gz 用完解压命令之后解压出的文件夹用户组是nogroup，使用一下命令更改所有者和用户组1chown -R root:root /usr/neo4j/neo4j-enterprise3.0.7 配置修改每个虚拟机上的neo4j配置文件1vi /usr/neo4j/neo4j-enterprise-3.0.7/conf/neo4j.conf 为个虚拟机的Neo4j配置文件修改图中三个部分： 1234567891011121314151617Neo4j-01：ha.mode=HAha.server_id=1ha.initial_hosts=IP1:5001,IP2:5001,IP3:5001Neo4j-02：ha.mode=HAha.server_id=2ha.initial_hosts=IP1:5001,IP2:5001,IP3:5001Neo4j-03：ha.mode=HAha.server_id=3ha.initial_hosts=IP1:5001,IP2:5001,IP3:5001 每个虚拟机启动Neo4j1/usr/neo4j/neo4j-enterprise-3.0.7/bin/neo4j start 至此一个具有一个主机两个从机的Neo4j集群就搭建完成了。 Neo4j集群详细配置 dbms.ha: 数据库的运行模式（HA/ARBITER）； ha.server_id: 集群中每个实例的唯一标识。必须是一个唯一的正整数； ha.host.coordination: 集群中的每个实例用来监听集群通信的端口，默认5001； ha.initial_hosts: 当每个实例启动时，帮助他们寻找和加入集群的端口。当集群刚启动时，在所有实例都加入到集群并且可以互相通信之前，这段时间数据库不允许被访问； ha.host.data: 用来监听集群中从Master提交过来的事物操作，默认为6001。不能与ha.host.coordination相同； 仲裁实例: 仲裁实例不同于Master实例和Slave实例，它工作在仲裁模式，虽然参与集群的通信，但是不复制集群中的数据存储，主要作用是打破Matser选举过程的死结，开启仲裁实例dbms.mode=ARBITER Master选举选举规则 开启集群时第一个启动的实例就是Master实例。 如果Master实例因为某种原因宕机，或者是集群冷启动，则Slave实例中提交事务最多的一个将会被选举为Master。 如果有两个Slave实例提交的事务一样多，那么host_server_id靠前的一个将赢得选举。 查看当前实例状态输入以下命令，如果返回true则是Master实例1curl http://&lt;your ip&gt;:7474/db/manage/server/ha/master 输入以下命令，如果返回true则是Slave实例1curl http://&lt;your ip&gt;:7474/db/manage/server/ha/slave 输入以下命令，如果返回master则是Master实例，返回slave则是Slave实例1curl http://&lt;your ip&gt;:7474/db/manage/server/ha/available 详细情况如下图所示： 本文结束。]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Lucene学习【入门】]]></title>
    <url>%2F2016%2F11%2F18%2Flucene%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[全文检索简介全文检索是将存储文件的全部内容都进行检索，也许是一本书，也许是一篇文章。他可以根据需要获得全文中的有关文章、节、段、句、词等信息，形象的表示就是给整本书的每个字词添加一个标签，以进行各种统计分析。Lucene全文检索就是基于这种应用的，Lucene采用“倒排索引”建立关键词与文件的相关映射，如下如所示： Lucene的核心jar包 lucene-core-5.5.0.jar: 其中包括了常用的文档，索引，搜索，存储等相关核心代码； lucene-analyzers-common-5.5.0.jar: 包括了各种词法分析器，用于对文件内容关键词进行切分，提取（但是只包含英文的）； lucene-queryparser-5.5.0.jar: 提取了搜索相关的代码，用于各种搜索，比如模糊搜索，范围搜索，等等 主要开发包说明 org.apache.lucene.analysis：语言分析器，主要用于分词； org.apache.lucene.document：索引文档的管理； org.apache.lucene.index：索引管理，如增、删、改； org.apache.lucene.queryparser：查询分析； org.apache.lucene.search：检索管理； org.apache.lucene.store：数据存储管理； org.apache.lucene.util：工具包 索引操作的核心类 Directory: 代表索引文档的存储位置，这是一个抽象类，有两个子类FSDirectory和RAMDriectory两个主要子类。前者将索引写入文件系统，后者将索引写入内存； Analyzer: 建立索引时所使用的分析器，在一个文档被索引之前对文档内容进行分词处理，主要子类有StandardAnalyzer(对汉字的分词是一个汉字一个词)。要想使用针对汉字的分析器，需要额外引入IKAnalyzer，PaodingAnalyzer等； IndexWriterConfig: 操作索引库配置信息； IndexWriter: 建立索引的核心类，用来操作索引(增，删，改)； Document: 代表一个索引文档，这里的文档可以指一篇文章，一个HTML页面等。一个Document可以由多个Field对象组成。可以把一个Document想成数据库中的一个记录，而每个Field就是记录的一个字段； Field: 代表索引文档中存储的数据，用来描述文档的某个属性，比如一封电子邮件的内容和标题可以用两个Field对象分别描述 写入索引用maven导入依赖的jar包123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt;&lt;/dependency&gt; 写入索引123456789101112131415161718192021222324//选择语言分析器Analyzer analyzer = new StandardAnalyzer();//索引文档的存储位置Directory directory = FSDirectory.open(Paths.get("./index"));//配置索引库信息IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer);//创建IndexWriter，用来进行索引文件的写入IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); //准备写入Document的文档String[] texts = new String[]&#123; "Apache Lucene is a high-performance", "full-featured text search engine library written entirely in Java", "It is a technology suitable for nearly any application that requires full-text search.", "Apache Lucene is an open source project available for free download. ", "Please use the links on the right to access Lucene."&#125;;//建立索引文档for(String text: texts)&#123; Document document = new Document(); document.add(new TextField("info", text, Field.Store.YES)); indexWriter.addDocument(document);&#125;indexWriter.close();directory.close(); 查询索引的核心类 IndexReader: 读取索引的工具类，常见的子类有DirectoryReader； IndexSeracher: 查询索引的核心类； QueryPaerser: 查询分析器，表示从哪里查用哪个分析器查； Query: 代表一次查询； TopDocs: 封装了匹配情况,旧版本用的是Hits，新版本已经弃用； ScoreDocs: 匹配情况的数据，里面封装了索引文档得分和索引ID 查询索引12345678910111213141516171819Analyzer analyzer = new StandardAnalyzer();Directory directory = FSDirectory.open(Paths.get("./index"));//建立IndexReaderIndexReader reader = DirectoryReader.open(directory);IndexSearcher indexSearcher = new IndexSearcher(reader);QueryParser queryParser = new QueryParser("info", analyzer);Query query = queryParser.parse("lucene");TopDocs topDocs = indexSearcher.search(query, 1000);System.out.println("总共匹配多少个：" + topDocs.totalHits);ScoreDoc[] hits = topDocs.scoreDocs;System.out.println("总共多少条数据：" + hits.length);for(ScoreDoc hit: hits)&#123; System.out.println("匹配的分：" + hit.score); System.out.println("文档索引id：" + hit.doc); Document document = indexSearcher.doc(hit.doc); System.out.println(document.get("info"));&#125;reader.close();directory.close(); 以上就是两个简单的全文索引建立和查找的例子，但是已经涵盖了主要的思想。]]></content>
      <categories>
        <category>Lucene</category>
      </categories>
      <tags>
        <tag>Lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j批量导入数据BatchInserter]]></title>
    <url>%2F2016%2F11%2F12%2FNeo4j%E6%89%B9%E9%87%8F%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AEBatchInserter%2F</url>
    <content type="text"><![CDATA[BatcherInserter的使用当需要向Neo4j大批量导入数据的时候，使用Neo4j核心API，Cypher和LOAD CSV等导入方式速度会很慢，因为这些导入方式会产生很多IO操作。 BatchInserter采用批量导入的方式，减少IO操作以提升导入速度，但是牺牲了实时性和事物(Transcations)支持，需要关闭数据库才可以。 12File dbPath = new File("D:/Neo4jDB/importTest");BatchInserter inserter = BatchInserters.inserter(new File(dbPath));]]></content>
      <categories>
        <category>Neo4j</category>
      </categories>
      <tags>
        <tag>Neo4j</tag>
        <tag>BatchInserter</tag>
        <tag>Neo4j批量导入</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令小结]]></title>
    <url>%2F2016%2F10%2F08%2FLinux%E6%9F%A5%E7%94%A8%E5%91%BD%E4%BB%A4%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[网络编程命令iptables命令traceroute命令telnet命令文件操作命令rm命令mkdir命令touch命令cp命令mv命令cd命令tar命令ls命令文本操作命令vi命令vim命令cat命令cat命令主要用来查看文件内容，创建文件，文件合并，追加文件内容等功能。 查看文件内容主要用法： cat f1.txt，查看f1.txt文件的内容。 cat -n f1.txt，查看f1.txt文件的内容，并且由1开始对所有输出行进行编号。 cat -b f1.txt，查看f1.txt文件的内容，用法与-n相似，只不过对于空白行不编号。 cat -s f1.txt，当遇到有连续两行或两行以上的空白行，就代换为一行的空白行。 cat -e f1.txt，在输出内容的每一行后面加一个$符号。 cat f1.txt f2.txt，同时显示f1.txt和f2.txt文件内容，注意文件名之间以空格分隔，而不是逗号。 cat -n f1.txt&gt;f2.txt，对f1.txt文件中每一行加上行号后然后写入到f2.txt中，会覆盖原来的内容，文件不存在则创建它。 cat -n f1.txt&gt;&gt;f2.txt，对f1.txt文件中每一行加上行号后然后追加到f2.txt中去，不会覆盖原来的内容，文件不存在则创建它。 sort命令sort主要用来对File参数指定的文件中的行排序，并将结果写到标准输出。如果File参数指定多个文件，那么sort命令将这些参数连接起来，当做一个文件进行排序。 12345678910[root@www ~]# sort [-fbMnrtuk] [file or stdin]选项与参数： -f ：忽略大小写的差异，例如 A 与 a 视为编码相同； -b ：忽略最前面的空格符部分； -M ：以月份的名字来排序，例如 JAN, DEC 等等的排序方法； -n ：使用『纯数字』进行排序(默认是以文字型态来排序的)； -r ：反向排序； -u ：就是 uniq ，相同的数据中，仅出现一行代表； -t ：分隔符，默认是用 [tab] 键来分隔； -k ：以那个区间 (field) 来进行排序的意思 对/etc/passwd 的账号进行排序 root@ubuntu-03:~# cat /etc/passwd | sort sort 是默认以第一个数据来排序，而且默认是以字符串形式来排序,所以由字母 a 开始升序排序。 /etc/passwd 内容是以 : 来分隔的，我想以第三栏来排序，该如何 root@ubuntu-03:~# cat /etc/passwd | sort -t “:” -k 3 默认是以字符串来排序的，如果想要使用数字排序： root@ubuntu-03:~# cat /etc/passwd | sort -t “:” -k 3n 默认是升序排序，如果要倒序排序，如下 root@ubuntu-03:~# cat /etc/passwd | sort -t ‘:’ -k 3nr uniq命令uniq命令可以去除排序过的文件中的重复行，因此uniq经常和sort合用。也就是说，为了使uniq起作用，所有的重复行必须是相邻的。12345[root@www ~]# uniq [-icu]选项与参数：-i ：忽略大小写字符的不同；-c ：进行计数-u ：只显示唯一的行 假设testfile内容如下： 1234567cat testfilehelloworldfriendhelloworldhello 直接删除未经排序的文件，将会发现没有任何行被删除1234567#uniq testfilehelloworldfriendhelloworldhello 排序文件，默认去重1234#cat words | sort |uniqfriendhelloworld 排序之后删除了重复行，同时在行首位置输出该行重复的次数1234#sort testfile | uniq -c1 friend3 hello2 world cut命令cut命令可以从一个文本文件或者文本流中提取文本列。 cut语法：123456[root@www ~]# cut -d&apos;分隔字符&apos; -f fields &lt;==用于有特定分隔字符[root@www ~]# cut -c 字符区间 &lt;==用于排列整齐的信息选项与参数：-d ：后面接分隔字符。与 -f 一起使用；-f ：依据 -d 的分隔字符将一段信息分割成为数段，用 -f 取出第几段的意思；-c ：以字符 (characters) 的单位取出固定字符区间； PATH变量如下12root@ubuntu-03:~# echo $PATH/usr/java/jdk/bin:/usr/java/jdk/jre/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 将 PATH 变量取出，我要找出第五个路径。12root@ubuntu-03:~# echo $PATH | cut -d &apos;:&apos; -f 5/usr/sbin 将 PATH 变量取出，我要找出第三到最后一个路径。12root@ubuntu-03:~# echo $PATH | cut -d &apos;:&apos; -f 3-/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin 将 PATH 变量取出，我要找出第一到第三个路径。12root@ubuntu-03:~# echo $PATH | cut -d &apos;:&apos; -f 1-3/usr/java/jdk/bin:/usr/java/jdk/jre/bin:/usr/local/sbin wc命令统计文件里面有多少单词，多少行，多少字符。wc语法：12345[root@www ~]# wc [-lwm]选项与参数：-l ：仅列出行；-w ：仅列出多少字(英文单字)；-m ：多少字符； 默认使用wc统计/etc/passwd12root@ubuntu-03:~# wc /etc/passwd 30 42 1561 /etc/passwd 40是行数，45是单词数，1719是字节数 wc的命令比较简单使用，每个参数使用如下：123456root@ubuntu-03:~# wc -l /etc/passwd30 /etc/passwd #统计行数root@ubuntu-03:~# wc -w /etc/passwd42 /etc/passwd #统计单词数root@ubuntu-03:~# wc -m /etc/passwd1561 /etc/passwd #统计文件字节数 grep搜索命令Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹 配的行打印出来。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 awk命令awk是一个强大的文本分析工具，相对于grep的查找，sed的编辑，awk在其对数据分析并生成报告时，显得尤为强大。简单来说awk就是把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行各种分析处理。 awk有3个不同版本: awk、nawk和gawk，未作特别说明，一般指gawk，gawk 是 AWK 的 GNU 版本。 awk其名称得自于它的创始人 Alfred Aho 、Peter Weinberger 和 Brian Kernighan 姓氏的首个字母。实际上 AWK 的确拥有自己的语言： AWK 程序设计语言 ， 三位创建者已将它正式定义为“样式扫描和处理语言”。它允许您创建简短的程序，这些程序读取输入文件、为数据排序、处理数据、对输入执行计算以及生成报表，还有无数其他的功能。 使用方法1awk '&#123;pattern + action&#125;' &#123;filenames&#125; 尽管操作可能会很复杂，但语法总是这样，其中 pattern 表示 AWK 在数据中查找的内容，而 action 是在找到匹配内容时所执行的一系列命令。花括号（{}）不需要在程序中始终出现，但它们用于根据特定的模式对一系列指令进行分组。 pattern就是要表示的正则表达式，用斜杠括起来。 awk语言的最基本功能是在文件或者字符串中基于指定规则浏览和抽取信息，awk抽取信息后，才能进行其他文本操作。完整的awk脚本通常用来格式化文本文件中的信息。 通常，awk是以文件的一行为处理单位的。awk每接收文件的一行，然后执行相应的命令，来处理文本。 调用awk123456789101112131.命令行方式awk [-F field-separator] &apos;commands&apos; input-file(s)其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。2.shell脚本方式将所有的awk命令插入一个文件，并使awk程序可执行，然后awk命令解释器作为脚本的首行，一遍通过键入脚本名称来调用。相当于shell脚本首行的：#!/bin/sh可以换成：#!/bin/awk3.将所有的awk命令插入一个单独文件，然后调用：awk -f awk-script-file input-file(s)其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的 ###入门实例显示最近登录的5个帐号123456root@ubuntu-03:~# last -n 5 | awk &apos;&#123;print $1&#125;&apos;rootrootrootwtmp awk工作流程是这样的：读入有’\n’换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域。默认域分隔符是”空白键” 或 “[tab]键”,所以$1表示登录用户，$3表示登录用户ip,以此类推 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以tab键分割123456root@ubuntu-03:~# cat /etc/passwd | awk -F &apos;:&apos; &apos;&#123;print $1&quot;\t&quot;$7&#125;&apos;root /bin/bashdaemon /usr/sbin/nologinbin /usr/sbin/nologinsys /usr/sbin/nologinsync /bin/sync 如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加”blue,/bin/nosh”。12345678root@ubuntu-03:~# cat /etc/passwd | awk -F &apos;:&apos; &apos;BEGIN&#123;print &quot;name, shell&quot;&#125;&#123;print $1&quot;\t&quot;$7&#125;END&#123;print &quot;blue,/bin/bash&quot;&#125;&apos;name, shellroot /bin/bashdaemon /usr/sbin/nologinbin /usr/sbin/nologinsys /usr/sbin/nologinsync /bin/syncblue, /bin/bash awk工作流程是这样的：先执行BEGING，然后读取文件，读入有/n换行符分割的一条记录，然后将记录按指定的域分隔符划分域，填充域，$0则表示所有域,$1表示第一个域,$n表示第n个域,随后开始执行模式所对应的动作action。接着开始读入第二条记录······直到所有的记录都读完，最后执行END操作。 搜索/etc/passwd有root关键字的所有行，并显示对应的shell12root@ubuntu-03:~# awk -F: &apos;/root/&#123;print $7&#125;&apos; /etc/passwd/bin/bash 这里指定了action{print $7} AWK内置变量1234567891011ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 统计/etc/passwd:文件名，每行的行号，每行的列数，对应的完整行内容:12345root@ubuntu-03:~# awk -F &apos;:&apos; &apos;&#123;print &quot;filename:&quot; FILENAME &quot;,linenumber:&quot; NR &quot;,columns:&quot; NF &quot;linecontent:&quot; $0&#125;&apos; /etc/passwdfilename:/etc/passwd,linenumber:1,columns:7linecontent:root:x:0:0:root:/root:/bin/bashfilename:/etc/passwd,linenumber:2,columns:7linecontent:daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologinfilename:/etc/passwd,linenumber:3,columns:7linecontent:bin:x:2:2:bin:/bin:/usr/sbin/nologinfilename:/etc/passwd,linenumber:4,columns:7linecontent:sys:x:3:3:sys:/dev:/usr/sbin/nologin 使用printf替代print,可以让代码更加简洁，易读1awk -F &apos;:&apos; &apos;&#123;printf(&quot;filename:%10s,linenumber:%s,columns:%s,linecontent:%s\n&quot;,FILENAME,NR,NF,$0)&#125;&apos; /etc/passwd awk编程下面统计/etc/passwd的账户人数12root@ubuntu-03:~# awk &apos;&#123;count++&#125; END&#123;print &quot;total is:&quot; count&#125;&apos; /etc/passwdtotal is:30 count是自定义变量。之前的action{}里都是只有一个print,其实print只是一个语句，而action{}可以有多个语句，以;号隔开。 这里没有初始化count，虽然默认是0，但是妥当的做法还是初始化为0:1awk &apos;BEGIN&#123;count=0; print &quot;count is:&quot; count&#125; &#123;count++&#125; END&#123;print &quot;total is:&quot; count&#125;&apos; /etc/passwd awk的循环语句awk中的循环语句同样借鉴于C语言，支持while、do/while、for、break、continue，这些关键字的语义和C语言中的语义完全相同。 awk的数组因为awk中数组的下标可以是数字和字母，数组的下标通常被称为关键字(key)。值和关键字都存储在内部的一张针对key/value应用hash的表格里。由于hash不是顺序存储，因此在显示数组内容时会发现，它们并不是按照你预料的顺序显示出来的。数组和变量一样，都是在使用时自动创建的，awk也同样会自动判断其存储的是数字还是字符串。一般而言，awk中的数组用来从记录中收集信息，可以用于计算总和、统计单词以及跟踪模板被匹配的次数等等。 显示/etc/passwd的账户1234567awk -F &apos;:&apos; &apos;BEGIN &#123;count=0;&#125; &#123;name[count] = $1;count++;&#125;; END&#123;for (i = 0; i &lt; NR; i++) print i, name[i]&#125;&apos; /etc/passwd0 root1 daemon2 bin3 sys4 sync5 games 权限操作命令Linux一般将文件可存取访问的身份分为3个类别：owner, group, others且三种身份各有read,write,execute等权限。 用户和用户组文件所有者可以设置除本人之外的用户无法查看文件的内容用户组假设A组有a1,a2,a3三个成员，B组有b1,b2成员，共同完成一份报告F。设置了适当的权限之后，A,B可以相互修改数据。C团体无法查看也无法修改。其他人（相对概念），比如f是A组以外的成员，那么f相对于A组成员就是其他人。超级用户root他拥有最大的权限，也管理普通用户相关文件在linux系统中，默认的系统账户和普通账户信息记录在/etc/passwd文件中 个人密码在/etc/shadow下 用户组名称记录在/etc/group下，所以这三个文件不能随便删除 linux文件权限的概念ls -la可查看文件详情12345drwxr-xr-x 4 root root 4096 Feb 21 19:18 neo4j | | | | | | | |文件权限 连接数 | | 文件大小 文件最后被修改时间 文件所有者 | 文件所属用户组 d rwx r-x r-x 中d代表文件类型，rwx代表文件所有者权限，r-x代表文件所属用户组权限，—代表其他人权限 r:read 可读 w:write 可写 x:execute 可执行 -：没有对应权限 改变文件属性的权限 chgrp：改变文件所属用户组 chown: 改变文件所有者 chmod: 改变文件权限 改变所属用户组chgrp(change group) 不过，要被改变的组名必须在/etc/group文件内才行，否则会出错。1chgrp [-R] 文件名/目录名 -R：进行递归，可以修改目录下的文件 改变文件所有者chown(change owner) 用户名必须存在于/etc/passwd中才行12chown [-R] 账号名称 文件名/目录名chown [-R] 账号名称：用户组名 文件名/目录名 改变权限chmod(change mode)有两种改变权限的方式： 1.数字类型改变文件权限 linux的基本权限有9个，即owner,group,others三中身份都有自己的r/w/x权限，各种权限对应的数字是：r-4,w-2,x-1。 例如：-rw-r–r– owner=rw-=4+2+0=6, group=r–=4+0+0=4,others=r–=4+0+0=412chmod [-R] xyz 文件名/目录名chmod 777 test.txt 2.符号类型改变权限u,g,o代表三中身份，a代表all，即全部身份 修改test.txt的权限为rwxr-xr-x:1chmod u=rwx,g=r-x,o=r-x test.txt 去掉test.txt所有身份的x权限1chmod a-x test-txt 添加test.txt所有身份的x权限1chmod a+x test.txt 状态查看命令远程操作命令ssh命令scp命令]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习笔记六]]></title>
    <url>%2F2016%2F08%2F03%2FZookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%2F</url>
    <content type="text"><![CDATA[本章开始学习下Curator一些典型的使用场景，可以为实际项目提供参考。 事件监听Zookeeper原声支持通过注册Watcher来进行事件监听，但使用起来不是特别方便，需要反复注册Watcher，比较繁琐。Curator引入了Cache来实现对Zookeeper服务端事件的监听，Cache是Curator中对事件监听的包装，并且自动反复注册监听。Cache分为两类监听类型：节点监听和子节点监听。 NodeCacheNodeCache用于监听指定Zookeeper数据节点本身的变化。 1234567891011121314151617181920212223242526272829303132333435public class NodeCacheSample &#123; static String path = &quot;/zk-book/nodecache&quot;; static CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;localhost:2181&quot;) .sessionTimeoutMs(5000) .retryPolicy(new ExponentialBackoffRetry(1000, 3)).build(); public static void main(String[] args) throws Exception &#123; client.start(); final NodeCache cache = new NodeCache(client, path, false); cache.start(true); cache.getListenable().addListener(() -&gt; &#123; System.out.println(&quot;NodeCacheListener...&quot;); if (cache.getCurrentData() != null) &#123; System.out.println(&quot;Node data update, new data: &quot; + new String(cache.getCurrentData().getData())); &#125; &#125;); //创建节点会触发NodeCacheListener client.create() .creatingParentsIfNeeded() .withMode(CreateMode.EPHEMERAL) .forPath(path, &quot;init&quot;.getBytes()); Thread.sleep(1000); /** * 修改节点会触发NodeCacheListener * 但是只会输出&quot;y&quot;，所以猜测NodeCache不适用并发修改场景 */ client.setData().forPath(path, &quot;x&quot;.getBytes()); client.setData().forPath(path, &quot;y&quot;.getBytes()); Thread.sleep(1000); //该版本删除节点会触发NodeCacheListener client.delete().deletingChildrenIfNeeded().forPath(path); Thread.sleep(1000); &#125;&#125; PathChildrenCachePathChildrenCache用于监听指定Zookeeper数据节点的子节点变化情况，无法对二级子节点进行事件监听。123456789101112131415161718192021222324252627282930313233343536373839public class PathChildrenCacheSample &#123; static String path = &quot;/zk-book&quot;; static CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;localhost:2181&quot;) .retryPolicy(new ExponentialBackoffRetry(1000, 3)) .sessionTimeoutMs(5000).build(); public static void main(String[] args) throws Exception &#123; client.start(); PathChildrenCache cache = new PathChildrenCache(client, path, true); cache.start(StartMode.POST_INITIALIZED_EVENT); cache.getListenable().addListener((client1, event) -&gt; &#123; switch (event.getType()) &#123; case CHILD_ADDED: System.out.println(&quot;CHILD_ADDED,&quot; + event.getData().getPath()); break; case CHILD_UPDATED: System.out.println(&quot;CHILD_UPDATED,&quot; + event.getData().getPath()); break; case CHILD_REMOVED: System.out.println(&quot;CHILD_REMOVED,&quot; + event.getData().getPath()); break; default: //CONNECTION_RECONNECTED、INITIALIZED System.out.println(event.getType()); break; &#125; &#125;); client.create().withMode(CreateMode.PERSISTENT).forPath(path); Thread.sleep(1000); //新增子节点会触发PathChildrenCacheListener client.create().withMode(CreateMode.PERSISTENT).forPath(path + &quot;/c1&quot;); Thread.sleep(1000); //删除子节点会触发PathChildrenCacheListener client.delete().forPath(path + &quot;/c1&quot;); Thread.sleep(1000); client.delete().forPath(path); Thread.sleep(1000); &#125;&#125; Master选举在分布式系统中，经常会碰到这样的场景：对于一个复杂的任务，仅需要从集群中选举出一台进行处理即可。诸如此类的分布式问题，我们统称为Master选举，借助Zookeeper可以轻松实现。其思路为：选在一个根节点，例如/master_select，多台机器同时想该节点创建一个子节点/master_select/lock，利用Zookeeper的特性，最终最有一台机器能够创建成功，那台机器就成为Master。 1234567891011121314151617181920212223242526public class RecipesMasterSelect &#123; static String master_path = &quot;/curator_recipes_master_path&quot;; static CuratorFramework client = CuratorFrameworkFactory.builder() .connectString(&quot;localhost:2181&quot;) .retryPolicy(new ExponentialBackoffRetry(1000, 3)).build(); public static void main(String[] args) throws Exception &#123; client.start(); LeaderSelector selector = new LeaderSelector(client, master_path, new LeaderSelectorListenerAdapter() &#123; /** * 一旦执行完takeLeadership方法，Curator就会立即释放Master权利，重新开始新一轮的Master选举 * @param client * @throws Exception */ public void takeLeadership(CuratorFramework client) throws Exception &#123; System.out.println(&quot;成为Master角色&quot;); Thread.sleep(3000); System.out.println(&quot;完成Master操作，释放Master权利&quot;); &#125; &#125;); selector.autoRequeue(); selector.start(); Thread.sleep(Integer.MAX_VALUE); &#125;&#125;]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper学习笔记五]]></title>
    <url>%2F2016%2F07%2F29%2Fzookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%2F</url>
    <content type="text"><![CDATA[Zookeeper提供了ALC的权限控制机制，简单来说就是通过设置Zookeeper服务器上数据节点的ACL，来控制客户端对该数据节点的访问权限。Zookeeper提供了多种权限控制模式，这里选择digest来了解下API的使用方法。 创建带权限信息的节点123456789101112131415161718//使用含权限信息的ZooKeeper会话创建数据节点public class ZNodeForFoo implements Watcher&#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception &#123; String path = &quot;/zk-book-auth_test&quot;; ZooKeeper zookeeper = new ZooKeeper(&quot;localhost:2181&quot;, 50000, new ZNodeForFoo()); connectedSemaphore.await(); //添加带权限信息的节点 zookeeper.addAuthInfo(&quot;digest&quot;, &quot;foo:true&quot;.getBytes()); zookeeper.create( path, &quot;init&quot;.getBytes(), Ids.CREATOR_ALL_ACL, CreateMode.EPHEMERAL ); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; if (Event.KeeperState.SyncConnected == watchedEvent.getState()) &#123; connectedSemaphore.countDown(); &#125; &#125;&#125; 使用不同的权限信息访问节点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//使用不同的权限信息的ZooKeeper会话访问含权限信息的数据节点public class GetFooNodeByAuth implements Watcher&#123; private static CountDownLatch noAuthSemaphore = new CountDownLatch(1); private static CountDownLatch wrongAuthSemaphore = new CountDownLatch(1); private static CountDownLatch rightAuthSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception &#123; String path = &quot;/zk-book-auth_test&quot;; try&#123; ZooKeeper noAuthZK = new ZooKeeper(&quot;localhost:2181&quot;, 50000, new GetFooNodeByAuth()); noAuthSemaphore.await(); //使用不包含权限信息的客户端访问节点，抛出异常 noAuthZK.getData( path, false, null ); &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; try&#123; ZooKeeper wrongAuthZK = new ZooKeeper(&quot;localhost:2181&quot;, 50000, new GetFooNodeByAuth()); wrongAuthSemaphore.await(); wrongAuthZK.addAuthInfo(&quot;digest&quot;, &quot;bar:true&quot;.getBytes()); //使用错误的权限信息访问节点，抛出异常 wrongAuthZK.getData( path, false, null ); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; ZooKeeper rightAuthZK = new ZooKeeper(&quot;localhost:2181&quot;, 50000, new GetFooNodeByAuth()); rightAuthSemaphore.await(); rightAuthZK.addAuthInfo(&quot;digest&quot;, &quot;foo:true&quot;.getBytes()); //使用正确的权限信息获取节点 System.out.println(new String(rightAuthZK.getData( path, false, null ))); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; if (Event.KeeperState.SyncConnected == watchedEvent.getState()) &#123; if(noAuthSemaphore.getCount() &gt; 0)&#123; noAuthSemaphore.countDown(); return; &#125; if(wrongAuthSemaphore.getCount() &gt; 0)&#123; wrongAuthSemaphore.countDown(); return; &#125; if(rightAuthSemaphore.getCount() &gt; 0)&#123; rightAuthSemaphore.countDown(); return; &#125; &#125; &#125;&#125; 删除节点的权限控制123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//删除节点的权限控制public class DeleteNodeByAuth implements Watcher&#123; private static CountDownLatch createSemaphore = new CountDownLatch(1); private static CountDownLatch deleteChildNoAuthSemaphore = new CountDownLatch(1); private static CountDownLatch deleteChildAuthSemaphore = new CountDownLatch(1); private static CountDownLatch deleteNoAuthSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception &#123; String path = &quot;/zk-book-auth_test&quot;; String pathChild = &quot;/zk-book-auth_test/child&quot;; ZooKeeper createZK = new ZooKeeper(&quot;localhost:2181&quot;,5000,new DeleteNodeByAuth()); createSemaphore.await(); createZK.addAuthInfo(&quot;digest&quot;, &quot;foo:true&quot;.getBytes()); createZK.create( pathChild, &quot;initChild&quot;.getBytes(), Ids.CREATOR_ALL_ACL, CreateMode.PERSISTENT ); try &#123; ZooKeeper deleteChildNoAuthZK = new ZooKeeper(&quot;localhost:2181&quot;,50000,new DeleteNodeByAuth()); deleteChildNoAuthSemaphore.await(); deleteChildNoAuthZK.delete( pathChild, -1 ); &#125; catch ( Exception e ) &#123; System.out.println( &quot;删除节点失败: &quot; + e.getMessage() ); &#125; ZooKeeper deleteChildAuthZK = new ZooKeeper(&quot;localhost:2181&quot;,50000,new DeleteNodeByAuth()); deleteChildAuthSemaphore.await(); deleteChildAuthZK.addAuthInfo(&quot;digest&quot;, &quot;foo:true&quot;.getBytes()); deleteChildAuthZK.delete( pathChild, -1 ); System.out.println( &quot;成功删除节点：&quot; + pathChild ); /** * 删除权限作用的范围是子节点，所有不包含权限信息的客户端可以删除/zk-book-auth_test节点 */ ZooKeeper deleteNoAuthZK = new ZooKeeper(&quot;localhost:2181&quot;, 50000, new DeleteNodeByAuth()); deleteNoAuthSemaphore.await(); deleteNoAuthZK.delete( path, -1 ); System.out.println( &quot;成功删除节点：&quot; + path ); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; if (Event.KeeperState.SyncConnected == watchedEvent.getState()) &#123; if(createSemaphore.getCount() &gt; 0)&#123; createSemaphore.countDown(); return; &#125; if(deleteChildNoAuthSemaphore.getCount() &gt; 0)&#123; deleteChildNoAuthSemaphore.countDown(); return; &#125; if(deleteChildAuthSemaphore.getCount() &gt; 0)&#123; deleteChildAuthSemaphore.countDown(); return; &#125; if(deleteNoAuthSemaphore.getCount() &gt; 0)&#123; deleteNoAuthSemaphore.countDown(); return; &#125; &#125; &#125;&#125;]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习笔记二]]></title>
    <url>%2F2016%2F07%2F27%2FZookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Zookeeper官网下载下载最新的安装包，目前最新的稳定版是Release 3.4.9(stable)，本课题所有示例都将基于这个版本，整体环境如下： 电脑环境：Ubuntu 16.04 JDK：1.8 Zookeeper：Release 3.4.9(stable) 安装Zookeeper1、创建目录，安装zookeeper手上只有一台测试机器，所以采用伪集群的方式运行zookeeper，下面创建三个文件夹server1、server2、server3分别用于安装zookeeper。123456789cd /optsudo mkdir zookeeper-3.4.9sudo chmod -R 777 zookeeper-3.4.9cd zookeeper-3.4.9cp ~/Downloads/zookeeper-3.4.9.tar.gz ./mkdir server1 &amp;&amp; tar -zxvf zookeeper-3.4.9.tar.gz -C ./server1 --strip-components 1mkdir server2 &amp;&amp; tar -zxvf zookeeper-3.4.9.tar.gz -C ./server2 --strip-components 1mkdir server3 &amp;&amp; tar -zxvf zookeeper-3.4.9.tar.gz -C ./server3 --strip-components 1rm zookeeper-3.4.9.tar.gz 2、修改配置文件123456789101112131415161718192021222324252627282930313233343536#server1mkdir ./server1/datacp server1/conf/zoo_sample.cfg server1/conf/zoo.cfgvim server1/conf/zoo.cfg #需要修改的属性如下 dataDir=/opt/zookeeper-3.4.9/server1/data clientPort=2181 server.1=localhost:2888:3888 server.2=localhost:2889:3889 server.3=localhost:2890:3890#创建与server对应的myidecho &quot;1&quot; &gt; server1/data/myid#server2mkdir server2/datacp server2/conf/zoo_sample.cfg server2/conf/zoo.cfgvim server2/conf/zoo.cfg #需要修改的属性如下 dataDir=/opt/zookeeper-3.4.9/server2/data clientPort=2182 server.1=localhost:2888:3888 server.2=localhost:2889:3889 server.3=localhost:2890:3890#创建与server对应的myidecho &quot;2&quot; &gt; server2/data/myid#server3mkdir server3/datacp server3/conf/zoo_sample.cfg server3/conf/zoo.cfgvim server3/conf/zoo.cfg #需要修改的属性如下 dataDir=/opt/zookeeper-3.4.9/server3/data clientPort=2183 server.1=localhost:2888:3888 server.2=localhost:2889:3889 server.3=localhost:2890:3890#创建与server对应的myidecho &quot;3&quot; &gt; server3/data/myid 3、启动服务测试下图显示信息表示服务启动正常： 客户端脚本客户端脚本的测试基于上面安装的Zookeeper服务，之后的所有测试示例同样基于上述服务。 创建节点使用create命令可以创建一个节点：create /first-node zhaobing，表示在Zookeeper的根节点下创建了一个/first-node的节点，并且节点的数据内容是”bboyjing”。 读取节点使用ls命令，可以列出指定节点下的所有子节点：ls /，表示查看根节点下的所有子节点。输出：[first-node, zookeeper]，first-node是之前创建的节点，zookeeper节点是自带的保留节点。使用get命令，可以获取指定节点的数据内容和属性信息：get /first-node 更新节点使用set命令，可以更新指定节点的数据内容：set /first-node hello-bboyjing 删除节点使用delete命令，可以删除指定节点：delete /first-node，要注意的是指定节点没有子节点才可以被删除 注意上述的命令只是简单的演示，每个命令还会有其他参数，比如创建节点时可以选择永久节点还是临时节点，更新的时候可以基于某个version之类的，这里就不展开了。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper学习笔记三【java客户端API的使用】]]></title>
    <url>%2F2016%2F07%2F26%2Fzookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%2F</url>
    <content type="text"><![CDATA[Zookeeper提供了源生Java Api，下面我们新建个项目来测试，之后所有的测试代码都放于该项目中，项目地址为zookeeper-sample。 1234567891011121314151617&lt;!-- Zookeeper --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.9&lt;/version&gt; &lt;!-- Multiple SLF4J bindings error--&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 创建会话，连接服务端123456789101112131415161718192021222324252627282930//Java API -&gt; 创建连接 -&gt; 创建一个最基本的ZooKeeper对象实例public class ZooKeeperConstructorUsageSimple implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception&#123; /** * Zookeeper客户端和服务端会话的建立是一个异步的过程 * 也就是说在程序中，构造方法会在处理完客户端初始化工作后立即返回 * 在大多数情况下此时并没有真正建立好一个可用的会话，此时在会话的生命周期中处于“CONNECTING”的状态 */ ZooKeeper zookeeper = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new ZooKeeperConstructorUsageSimple()); System.out.println(zookeeper.getState()); try &#123; //等待Watcher通知SyncConnected connectedSemaphore.await(); &#125; catch (InterruptedException e) &#123;&#125; System.out.println(&quot;ZooKeeper session established.&quot;); &#125; /** * ZooKeeper_Constructor_Usage_Simples实现了Watcher接口，重写了process方法 * 该方法负责处理来自Zookeeper服务端的Watcher通知，即服务端建立连接后会调用该方法 * @param event */ public void process(WatchedEvent event) &#123; System.out.println(&quot;Receive watched event：&quot; + event); if (Event.KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//Java API -&gt; 创建连接 -&gt; 创建一个最基本的ZooKeeper对象实例，复用sessionId和passwordpublic class ZooKeeperConstructorUsageWithSidPassword implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception&#123; ZooKeeper zookeeper = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new ZooKeeperConstructorUsageWithSidPassword()); connectedSemaphore.await(); /** * 获取sessionId、password，目的是为了复用会话 */ long sessionId = zookeeper.getSessionId(); byte[] password = zookeeper.getSessionPasswd(); //使用错误的sessionId和password连接 zookeeperConnector wrong = new zookeeperConnector(1, &quot;test&quot;.getBytes(), new CountDownLatch(1)); wrong.connect(); //使用正确的sessionId和password连接 zookeeperConnector correct = new zookeeperConnector(sessionId, password, new CountDownLatch(1)); correct.connect(); &#125; public void process(WatchedEvent event) &#123; System.out.println(&quot;Receive watched event：&quot; + event); if (KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; &#125; static class zookeeperConnector implements Watcher&#123; private long sessionId; private byte[] password; private CountDownLatch connectedSemaphore; public zookeeperConnector(long sessionId, byte[] password, CountDownLatch connectedSemaphore)&#123; this.sessionId = sessionId; this.password = password; this.connectedSemaphore = connectedSemaphore; &#125; public void connect() throws IOException, InterruptedException &#123; new ZooKeeper(&quot;localhost:2181&quot;, 5000, this, sessionId, password); this.connectedSemaphore.await(); &#125; @Override public void process(WatchedEvent watchedEvent) &#123; System.out.println(&quot;Receive watched event：&quot; + watchedEvent); this.connectedSemaphore.countDown(); &#125; &#125;&#125;//从输出中截取出三条能代表结果的信息如下：Receive watched event：WatchedEvent state:SyncConnected type:None path:nullReceive watched event：WatchedEvent state:Expired type:None path:nullReceive watched event：WatchedEvent state:SyncConnected type:None path:null 创建节点创建节点的API分同步和异步两种方式，无论时同步还是异步接口，Zookeeper都不支持递归创建。 1234567891011121314151617181920212223242526272829303132333435363738//ZooKeeper API创建节点，使用同步(sync)接口。public class ZooKeeperCreateAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception&#123; ZooKeeper zookeeper = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new ZooKeeperCreateAPISyncUsage()); connectedSemaphore.await(); /** * 创建临时节点 */ String path1 = zookeeper.create(&quot;/zk-test-ephemeral-&quot;, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(&quot;Success create znode: &quot; + path1); /** * 创建临时顺序节点 * Zookeeper会自动在节点后缀加上一个数字 */ String path2 = zookeeper.create(&quot;/zk-test-ephemeral-&quot;, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(&quot;Success create znode: &quot; + path2); /** * 创建永久节点 */ String path3 = zookeeper.create(&quot;/persistent-node&quot;,&quot;bboyjing&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(&quot;Success create znode: &quot; + path3); &#125; public void process(WatchedEvent event) &#123; if (KeeperState.SyncConnected == event.getState()) &#123; connectedSemaphore.countDown(); &#125; &#125;&#125;//在zookeeper客户端中查看结果[zk: localhost:2181(CONNECTED) 7] ls /[zookeeper, persistent-node][zk: localhost:2181(CONNECTED) 8] get /persistent-nodezhaobing 删除节点删除节点API分同步和异步两种方式，只允许删除叶子节点。 1234567891011121314151617181920// ZooKeeper API 删除节点，使用同步(sync)接口。public class DeleteAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception &#123; ZooKeeper zk = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new DeleteAPISyncUsage()); connectedSemaphore.await(); /** * 删除节点，需要注意的是至允许删除叶子节点 */ zk.delete(&quot;/persistent-node&quot;, -1); &#125; @Override public void process(WatchedEvent event) &#123; if (KeeperState.SyncConnected == event.getState()) &#123; if (EventType.None == event.getType() &amp;&amp; null == event.getPath()) &#123; connectedSemaphore.countDown(); &#125; &#125; &#125;&#125; 读取节点数据读取的数据包括子节点列表和节点数据，Zookeeper提供了不同的API，而且还能注册Watcher来订阅节点相关信息的变化。 读取子节点(getChildren)123456789101112131415161718192021222324252627282930313233343536373839404142434445// ZooKeeper API 获取子节点列表，使用同步(sync)接口。public class ZooKeeperGetChildrenAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private static CountDownLatch watcherSemaphore = new CountDownLatch(1); private static ZooKeeper zk = null; public static void main(String[] args) throws Exception&#123; /** * 声明node路径 * 实例化Zookeeper */ String path = &quot;/zk-book&quot;; zk = new ZooKeeper(&quot;localhost:2181&quot;, 500000, new ZooKeeperGetChildrenAPISyncUsage()); connectedSemaphore.await(); /** * 创建永久节点/zk-book * 创建临时节点/zk-book/c1 */ zk.create(path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); zk.create(path + &quot;/c1&quot;, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); /** * 获取/zk-book下的子节点 * 此时注册了默认的watch，如果继续在/zk-book下增加节点的话，会调用process方法，通知客户端节点变化了 * 但是仅仅是发通知，客户端需要自己去再次查询 * 另外需要注意的是watcher是一次性的，即一旦触发一次通知后，该watcher就失效了，需要反复注册watcher， * 即process方中的getChildren继续注册了watcher */ List&lt;String&gt; childrenList = zk.getChildren(path, true); System.out.println(childrenList); zk.create(path+&quot;/c2&quot;, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); watcherSemaphore.await(); &#125; public void process(WatchedEvent event) &#123; if (KeeperState.SyncConnected == event.getState()) &#123; if (EventType.None == event.getType() &amp;&amp; null == event.getPath()) &#123; connectedSemaphore.countDown(); &#125; else if (event.getType() == EventType.NodeChildrenChanged) &#123; try &#123; //收到子节点变更通知，重新主动查询子节点信息 System.out.println(&quot;ReGet Child:&quot; + zk.getChildren(event.getPath(),true)); watcherSemaphore.countDown(); &#125; catch (Exception e) &#123;&#125; &#125; &#125; &#125;&#125; 获取节点数据(getData)12345678910111213141516171819202122232425262728293031323334353637383940// ZooKeeper API 获取节点数据内容，使用同步(sync)接口。public class GetDataAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private static CountDownLatch nodeDataChangedSemaphore = new CountDownLatch(1); private static ZooKeeper zk = null; private static Stat stat = new Stat(); public static void main(String[] args) throws Exception &#123; String path = &quot;/zk-book&quot;; zk = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new GetDataAPISyncUsage()); connectedSemaphore.await(); /** * 新增节点并给节点赋值 */ zk.create( path, &quot;123&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL ); /** * 获取节点数据，传入旧的stat，会被服务端响应的新的stat替换 */ System.out.println(new String(zk.getData( path, true, stat ))); System.out.println(stat.getCzxid()+&quot;,&quot;+stat.getMzxid()+&quot;,&quot;+stat.getVersion()); /** * 虽然节点的值没有改版，但是版本号改变了，依然会触发process事件 */ zk.setData( path, &quot;123&quot;.getBytes(), -1 ); nodeDataChangedSemaphore.await(); &#125; @Override public void process(WatchedEvent event) &#123; if (KeeperState.SyncConnected == event.getState()) &#123; if (EventType.None == event.getType() &amp;&amp; null == event.getPath()) &#123; connectedSemaphore.countDown(); &#125; else if (event.getType() == EventType.NodeDataChanged) &#123; try &#123; System.out.println(new String(zk.getData( event.getPath(), true, stat))); System.out.println(stat.getCzxid()+&quot;,&quot;+ stat.getMzxid()+&quot;,&quot;+ stat.getVersion()); nodeDataChangedSemaphore.countDown(); &#125; catch (Exception e) &#123;&#125; &#125; &#125; &#125;&#125; #更新数据1234567891011121314151617181920212223242526272829303132333435363738394041// ZooKeeper API 更新节点数据内容，使用同步(sync)接口。public class SetDataAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); public static void main(String[] args) throws Exception &#123; String path = &quot;/zk-book&quot;; ZooKeeper zk = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new SetDataAPISyncUsage()); connectedSemaphore.await(); zk.create( path, &quot;123&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL ); /** * version：-1，代表不需要根据版本号更新 */ Stat stat = zk.setData( path, &quot;456&quot;.getBytes(), -1 ); System.out.println(stat.getCzxid()+&quot;,&quot;+ stat.getMzxid()+&quot;,&quot;+ stat.getVersion()); /** * 根据上一次更新的版本号更新，成功 */ Stat stat2 = zk.setData( path, &quot;456&quot;.getBytes(), stat.getVersion()); System.out.println(stat2.getCzxid()+&quot;,&quot;+ stat2.getMzxid()+&quot;,&quot;+ stat2.getVersion()); /** * 根据上上次旧的版本跟新，失败抛异常 */ try &#123; zk.setData( path, &quot;456&quot;.getBytes(), stat.getVersion() ); &#125; catch ( KeeperException e ) &#123; System.out.println(&quot;Error: &quot; + e.code() + &quot;,&quot; + e.getMessage()); &#125; &#125; @Override public void process(WatchedEvent event) &#123; if (KeeperState.SyncConnected == event.getState()) &#123; if (EventType.None == event.getType() &amp;&amp; null == event.getPath()) &#123; connectedSemaphore.countDown(); &#125; &#125; &#125;&#125;异步更新方式在项目代码示例中，不贴出来了。 检测节点是否存在1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// ZooKeeper API 判断节点是否存在，使用同步(sync)接口。public class ExistAPISyncUsage implements Watcher &#123; private static CountDownLatch connectedSemaphore = new CountDownLatch(1); private static CountDownLatch lastSemaphore = new CountDownLatch(1); private static ZooKeeper zk; private static String path = &quot;/zk-book&quot;; public static void main(String[] args) throws Exception &#123; zk = new ZooKeeper(&quot;localhost:2181&quot;, 5000, new ExistAPISyncUsage()); connectedSemaphore.await(); /** * 通过exists接口检测是否存在指定节点，同事注册一个Watcher */ zk.exists( path, true ); /** * 创建节点/zk-book，服务器会向客户端发送事件通知：NodeCreated * 客户端收到通知后，继续调用exists接口，注册Watcher */ zk.create( path, &quot;&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT ); /** * 更新节点数据，服务器会向客户端发送事件通知：NodeDataChanged * 客户端收到通知后，继续调用exists接口，注册Watcher */ zk.setData( path, &quot;123&quot;.getBytes(), -1 ); /** * 删除节点/zk-book * 客户端会收到服务端的事件通知：NodeDeleted */ zk.delete( path, -1 ); lastSemaphore.await(); &#125; @Override public void process(WatchedEvent event) &#123; try &#123; if (KeeperState.SyncConnected == event.getState()) &#123; if (EventType.None == event.getType() &amp;&amp; null == event.getPath()) &#123; connectedSemaphore.countDown(); &#125; else if (EventType.NodeCreated == event.getType()) &#123; System.out.println(&quot;Node(&quot; + event.getPath() + &quot;)Created&quot;); zk.exists( event.getPath(), true ); &#125; else if (EventType.NodeDeleted == event.getType()) &#123; System.out.println(&quot;Node(&quot; + event.getPath() + &quot;)Deleted&quot;); zk.exists( event.getPath(), true ); System.out.println(&quot;Last semaphore&quot;); lastSemaphore.countDown(); &#125; else if (EventType.NodeDataChanged == event.getType()) &#123; System.out.println(&quot;Node(&quot; + event.getPath() + &quot;)DataChanged&quot;); zk.exists( event.getPath(), true ); &#125; &#125; &#125; catch (Exception e) &#123;&#125; &#125;&#125;异步方式我没有再写出来，自行查看API吧。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper学习笔记一]]></title>
    <url>%2F2016%2F07%2F20%2Fzookeeper%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[这两天随手翻了下以前看过的《从Paxos到Zookeeper分布式一致性原理与实践》，发现跟没看过一样。于是有了再看一遍的念头，顺便记下这一系列学习笔记。首先，向作者倪超致敬，笔记内容绝大部分来自于这本书，本章来过一过一些概念性的东西吧。 ACIDACID是传统单机事物的特征，分别是原子性(Atomicity)，一致性(Consistenty)，隔离性(Isolation)和持久性(Durability)，这个知识相信大家都很了解了，这里就稍微意思地梳理一下。 Atomicity 数据库能够进行操作的最小的逻辑单元，它包含的所有操作都是不可分割的 Consistenty 事务中包含的一系列的操作，这些操作必须同时成功，或者同时失败 Isolation 并发事物相互独立，当然得依赖事物隔离级别的类型 Durability 一旦事物成功结束，它对数据库所做的变更就必须永久保存下来 CAP一个分布式系统不可能同时满足一致性(C:Consistenty)、可用性(A:Availability)和分区容错性(P:Partition tolerance)这三个基本需求，最多只能同时满足其中的两项。 Consistenty 在分布式环境中，一致性是指数据在多个副本之间是否能够保持一致的特性 Availability 系统提供的服务必须一直处于可用的状态 Partition tolerance 分布式系统在遇到任何网络分区故障时，仍然能够保证对外提供一致性和可用性的服务上面已经说了，一个分布式系统无法同时满足上述三个需求，而只能满足其中的两项，因此就必须抛弃其中一项： 放弃CAP理论 说明 放弃P 放弃P意味着放弃了系统的可扩展性 放弃A 放弃A意味着一段时间系统无法对外提供正常的服务 放弃C 事实上放弃一致性指的是放弃数据的强一致性，而保留数据的最终一致性 一个分布式系统必然会面临网络问题，也就是说分区容错性是必须要面对的，因此系统架构师往往需要把精力花在如何根据业务特点在C(一致性)和A(可用性)之间寻求平衡。 BASEBASE理论是对CAP中一致性和可用性权衡的结果，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。 基本可用：是指分布式系统在出现不可预知故障的时候，允许损失部分可用性－－但是这绝不等价于系统不可用 弱状态：允许系统在不同节点的数据副本之间进行数据同步的过程存在延时 最终一致性：需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性 结束除了上面说的一些概念，还有3PC和Paxos算法等，这里就先不讲了，因为我也说不清楚。。。这一章就到这里了，下面还是重点学习学习如何在工程中使用Zookeeper。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习笔记六]]></title>
    <url>%2F2016%2F07%2F10%2Fspark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习笔记五]]></title>
    <url>%2F2016%2F06%2F15%2Fspark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>分布式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习笔记四]]></title>
    <url>%2F2016%2F06%2F08%2Fspark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9B%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>分布式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习笔记三]]></title>
    <url>%2F2016%2F06%2F05%2Fspark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>分布式计算</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark学习笔记二]]></title>
    <url>%2F2016%2F05%2F12%2FSpark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[本例子通过Spark sql 链接其他数据库。对数据库的信息进行操作。过滤。 首先是main 方法，创建SparkSession实例123456789101112131415def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setAppName(&quot;SparkSQLDemo&quot;) sparkConf.setMaster(&quot;local&quot;) val spark = SparkSession.builder().appName(&quot;SparkSQLDemo&quot;).config(sparkConf).getOrCreate() //创建数据库连接，从数据库查询数据，并存储本地。 runJDBCDataSource(spark) //从json文件中加载数据，进行搜索 loadDataSourceFromeJson(spark) //从parquet文件中加载数据，进行搜索 loadDataSourceFromeParquet(spark) 从RDD中加载数据 runFromRDD(spark) spark.stop() &#125; runJDBCDataSource链接数据库，操作数据，首先配置数据库连接信息。连接数据库，进行搜索，然后将数据输出到本地文件（此处我输出的文件路径相同。所以每次输出以后记得将文件改名，并将文件夹删除，要不然会报错文件路径已存在呢）12345678910111213141516171819private def runJDBCDataSource(spark: SparkSession): Unit = &#123; val jdbcDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/msm?user=root&amp;password=admin&quot;) //必须写表名 .option(&quot;dbtable&quot;, &quot;sec_user&quot;) .load() //查询数据库中的id, name, telephone三个列并以parquet（列存储）的方式存储在src/main/resources/sec_users路径下（存储后记得将名字改为user.parquet） //jdbcDF.select(&quot;id&quot;, &quot;name&quot;, &quot;telephone&quot;).write.format(&quot;parquet&quot;).save(&quot;src/main/resources/sec_users&quot;) //查询数据库中的username, name, telephone三个列并以parquet（列存储）的方式存储在src/main/resources/sec_users路径下存储后记得将名字改为user.json） //jdbcDF.select(&quot;username&quot;, &quot;name&quot;, &quot;telephone&quot;).write.format(&quot;json&quot;).save(&quot;src/main/resources/sec_users&quot;) //存储成为一张虚表user_abel jdbcDF.select(&quot;username&quot;, &quot;name&quot;, &quot;telephone&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;user_abel&quot;) val jdbcSQl = spark.sql(&quot;select * from user_abel where name like &apos;王%&apos; &quot;) jdbcSQl.show() jdbcSQl.write.format(&quot;json&quot;).save(&quot;./out/resulted&quot;) &#125; loadDataSourceFromeJson从runJDBCDataSource产生的json文件中读取数据进行处理并将结果存储。1234567891011121314private def loadDataSourceFromeJson(spark: SparkSession): Unit = &#123; //从runJDBCDataSource产生的user.json中读取数据 val jsonDF = spark.read.json(&quot;src/main/resources/user.json&quot;) //输出结构 jsonDF.printSchema() //创建临时视图 jsonDF.createOrReplaceTempView(&quot;user&quot;) //从临时视图进行查询 val namesDF = spark.sql(&quot;SELECT name FROM user WHERE name like &apos;王%&apos;&quot;) import spark.implicits._ //操作查询结果，在每个查询结果前加&quot;Name: &quot; 但使用该方法必须导入spark.implicits._ namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show()//将结果以json的形式写入到./out/resultedJSON 路径下 jsonDF.select(&quot;name&quot;).write.format(&quot;json&quot;).save(&quot;./out/resultedJSON&quot;) &#125; loadDataSourceFromeParquet从runJDBCDataSource产生的parquet(列式存储)文件中读取数据进行处理并将结果存储。12345678910private def loadDataSourceFromeParquet(spark: SparkSession): Unit = &#123; //从runJDBCDataSource产生的user.json中读取数据 val parquetDF = spark.read.load(&quot;src/main/resources/user.parquet&quot;) //创建临时视图 parquetDF.createOrReplaceTempView(&quot;user&quot;) val namesDF = spark.sql(&quot;SELECT name FROM user WHERE id &gt; 1 &quot;) namesDF.show()//将结果以parquet的形式写入到./out/resultedParquet 路径下 parquetDF.select(&quot;name&quot;).write.format(&quot;parquet&quot;).save(&quot;./out/resultedParquet&quot;) &#125; runFromRDD从RDD中读取数据进行搜索处理。12345678private def runFromRDD(spark: SparkSession): Unit = &#123; //创建一个json形式的RDD val otherPeopleRDD = spark.sparkContext.makeRDD( &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil) //从RDD中读取数据 val otherPeople = spark.read.json(otherPeopleRDD) otherPeople.show() &#125; 完整代码如下： 此处我输出的文件路径相同。所以每次输出以后记得将文件改名(gai为user.json和user.parquet)，并将文件夹删除，要不然会报错文件路径已存在呢. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import org.apache.spark.SparkConfimport org.apache.spark.sql.SparkSession/** * Created by yangyibo on 16/11/24. */object SparkSQLDemo &#123; def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setAppName(&quot;SparkSQLDemo&quot;) sparkConf.setMaster(&quot;local&quot;) val spark = SparkSession.builder().appName(&quot;SparkSQLDemo&quot;).config(sparkConf).getOrCreate() runJDBCDataSource(spark) loadDataSourceFromeJson(spark) loadDataSourceFromeParquet(spark) runFromRDD(spark) spark.stop() &#125; private def runJDBCDataSource(spark: SparkSession): Unit = &#123; val jdbcDF = spark.read .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/msm?user=root&amp;password=admin&quot;) .option(&quot;dbtable&quot;, &quot;sec_user&quot;) //必须写表名 .load() //jdbcDF.select(&quot;id&quot;, &quot;name&quot;, &quot;telephone&quot;).write.format(&quot;parquet&quot;).save(&quot;src/main/resources/sec_users&quot;) //jdbcDF.select(&quot;username&quot;, &quot;name&quot;, &quot;telephone&quot;).write.format(&quot;json&quot;).save(&quot;src/main/resources/sec_users&quot;) //存储成为一张虚表user_abel jdbcDF.select(&quot;username&quot;, &quot;name&quot;, &quot;telephone&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;user_abel&quot;) val jdbcSQl = spark.sql(&quot;select * from user_abel where name like &apos;王%&apos; &quot;) jdbcSQl.show() jdbcSQl.write.format(&quot;json&quot;).save(&quot;./out/resulted&quot;) &#125; private def loadDataSourceFromeJson(spark: SparkSession): Unit = &#123; //load 方法是加载parquet 列式存储的数据 // val jsonDF=spark.read.load(&quot;src/main/resources/sec_users/user.json&quot;) val jsonDF = spark.read.json(&quot;src/main/resources/user.json&quot;) jsonDF.printSchema() //创建临时视图 jsonDF.createOrReplaceTempView(&quot;user&quot;) val namesDF = spark.sql(&quot;SELECT name FROM user WHERE name like &apos;王%&apos;&quot;) import spark.implicits._ namesDF.map(attributes =&gt; &quot;Name: &quot; + attributes(0)).show() jsonDF.select(&quot;name&quot;).write.format(&quot;json&quot;).save(&quot;./out/resultedJSON&quot;) &#125; private def loadDataSourceFromeParquet(spark: SparkSession): Unit = &#123; val parquetDF = spark.read.load(&quot;src/main/resources/user.parquet&quot;) parquetDF.createOrReplaceTempView(&quot;user&quot;) val namesDF = spark.sql(&quot;SELECT name FROM user WHERE id &gt; 1 &quot;) namesDF.show() parquetDF.select(&quot;name&quot;).write.format(&quot;parquet&quot;).save(&quot;./out/resultedParquet&quot;) &#125; private def runFromRDD(spark: SparkSession): Unit = &#123; val otherPeopleRDD = spark.sparkContext.makeRDD( &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil) val otherPeople = spark.read.json(otherPeopleRDD) otherPeople.show() &#125;&#125;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark学习笔记一]]></title>
    <url>%2F2016%2F05%2F10%2Fspark%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[本例子是我初尝 Spark 的sparkStreaming官方小例子修改的。 我的思路是使用jdbc 链接数据库，然后查询数据库，将查询结果生成一个RDD ，放入RDD queue，然后每次取出rdd 进行计算和过滤处理。 1.sparkStreamingDemo由于这个demo需要spark 和jdbc 的依赖包。在pom.xml文件中如下（关于新建maven 的spark工程请参考idea 构建maven 管理的spark项目） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.us.demo&lt;/groupId&gt; &lt;artifactId&gt;mySpark&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;spark.version&gt;2.0.2&lt;/spark.version&gt; &lt;scala.version&gt;2.11&lt;/scala.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-hive_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- JDBC--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.12&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.19&lt;/version&gt; &lt;configuration&gt; &lt;skip&gt;true&lt;/skip&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; SparkStreamingDemo demo 的代码如下，我会尽量逐行增加注释：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import java.sql.&#123;DriverManager, ResultSet&#125;import scala.collection.mutable.Queueimport org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.rdd.RDDimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * Created by yangyibo on 16/11/23. */object SparkStreamingDemo &#123; def main(args: Array[String]) &#123; //创建spark实例 val sparkConf = new SparkConf().setAppName(&quot;QueueStream&quot;) sparkConf.setMaster(&quot;local&quot;) // 创建sparkStreamingContext ，Seconds是多久去Rdd中取一次数据。 val ssc = new StreamingContext(sparkConf, Seconds(3)) // Create the queue through which RDDs can be pushed to a QueueInputDStream var rddQueue = new Queue[RDD[String]]() // 从rdd队列中读取输入流 val inputStream = ssc.queueStream(rddQueue) //将输入流中的每个元素（每个元素都是一个String）后面添加一个“a“字符，并返回一个新的rdd。 val mappedStream = inputStream.map(x =&gt; (x + &quot;a&quot;, 1)) //reduceByKey(_ + _)对每个元素统计次数。map(x =&gt; (x._2,x._1))是将map的key和value 交换位置。后边是过滤次数超过1次的且String 相等于“testa“ val reducedStream = mappedStream.reduceByKey(_ + _) .map(x =&gt; (x._2,x._1)).filter((x)=&gt;x._1&gt;1).filter((x)=&gt;x._2.equals(&quot;testa&quot;)) reducedStream.print() //将每次计算的结果存储在./out/resulted处。 reducedStream.saveAsTextFiles(&quot;./out/resulted&quot;) ssc.start() //从数据库中查出每个用户的姓名，返回的是一个String有序队列seq，因为生成RDD的对象必须是seq。 val seq = conn() println(Seq) //将seq生成RDD然后放入Spark的Streaming的RDD队列，作为输入流。 for (i &lt;- 1 to 3) &#123; rddQueue.synchronized &#123; rddQueue += ssc.sparkContext.makeRDD(seq,10) &#125; Thread.sleep(3000) &#125; ssc.stop() &#125;//从数据库中取出每个用户的名字，是个String有序队列 def conn(): Seq[String] = &#123; val user = &quot;root&quot; val password = &quot;admin&quot; val host = &quot;localhost&quot; val database = &quot;msm&quot; val conn_str = &quot;jdbc:mysql://&quot; + host + &quot;:3306/&quot; + database + &quot;?user=&quot; + user + &quot;&amp;password=&quot; + password //classOf[com.mysql.jdbc.Driver] Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance(); val conn = DriverManager.getConnection(conn_str) var setName = Seq(&quot;&quot;) try &#123; // Configure to be Read Only val statement = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY) // Execute Query，查询用户表 sec_user 是我的用户表，有name属性。 val rs = statement.executeQuery(&quot;select * from sec_user&quot;) // Iterate Over ResultSet while (rs.next) &#123; // 返回行号 // println(rs.getRow) val name = rs.getString(&quot;name&quot;) setName = setName :+ name &#125; return setName &#125; finally &#123; conn.close &#125; &#125;&#125; 2.scala 链接mysql123456789101112131415161718192021222324252627282930313233343536373839import java.sql.&#123;Connection, DriverManager, ResultSet&#125;/** * Created by yangyibo on 16/11/23. */object DB &#123; def main(args: Array[String]) &#123; val user = &quot;root&quot; val password = &quot;admin&quot; val host = &quot;localhost&quot; val database = &quot;msm&quot; val conn_str = &quot;jdbc:mysql://&quot; + host + &quot;:3306/&quot; + database + &quot;?user=&quot; + user + &quot;&amp;password=&quot; + password println(conn_str) val conn = connect(conn_str) val statement =conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY); // Execute Query val rs = statement.executeQuery(&quot;select * from sec_user&quot;) // Iterate Over ResultSet while (rs.next) &#123; // 返回行号 // println(rs.getRow) val name = rs.getString(&quot;name&quot;) println(name) &#125; closeConn(conn) &#125; def connect(conn_str: String): Connection = &#123; //classOf[com.mysql.jdbc.Driver] Class.forName(&quot;com.mysql.jdbc.Driver&quot;).newInstance(); return DriverManager.getConnection(conn_str) &#125; def closeConn(conn:Connection): Unit =&#123; conn.close() &#125;&#125; scala 链接MySQL所需依赖1234567&lt;!-- JDBC--&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.12&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
</search>